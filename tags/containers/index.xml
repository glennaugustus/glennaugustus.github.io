<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Containers on glennaugustus.com</title><link>https://glennaugustus.com/tags/containers/</link><description>Recent content in Containers on glennaugustus.com</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 19 Aug 2015 13:27:52 +0000</lastBuildDate><atom:link href="https://glennaugustus.com/tags/containers/index.xml" rel="self" type="application/rss+xml"/><item><title>Compute Workload Abstraction - Part 5 or When Milli and Micro Meet</title><link>https://glennaugustus.com/post/compute-workload-abstraction-part-5-or-when-milli-and-micro-meet/</link><pubDate>Wed, 19 Aug 2015 13:27:52 +0000</pubDate><guid>https://glennaugustus.com/post/compute-workload-abstraction-part-5-or-when-milli-and-micro-meet/</guid><description>&lt;p>This is part five in a series exploring the topic of compute workload abstraction, in earlier posts we discovered how containers fit into the IT ecosystem. In this final part we take a bit of a tangent and look at performance and how adding the next level of abstraction inherits and obscures performance characteristics.&lt;/p>
&lt;h2 id="ok-who-invited-io-to-the-party">OK, who invited I/O to the party?&lt;/h2>
&lt;p>Sharing of a CPU and Memory resources are low hanging fruit from a virtualization perspective, mostly due to their predictable nature in terms of performance, and to some degree the closed nature of the interface to the operating system leading to further stability. Adding I/O sharing and prioritization has always been a challenge, and can be a bit complicated, so let’s look at it all with the analogy of a restaurant – and bear with me, as you may get hungry during the explanation.&lt;/p></description></item><item><title>Compute Workload Abstraction - Part 4 or The mental mettle to manage metal</title><link>https://glennaugustus.com/post/compute-workload-abstraction-part-4-the-mental-mettle-to-manage-metal/</link><pubDate>Wed, 12 Aug 2015 13:20:38 +0000</pubDate><guid>https://glennaugustus.com/post/compute-workload-abstraction-part-4-the-mental-mettle-to-manage-metal/</guid><description>&lt;p>This is part four in a series exploring the topic of compute workload abstraction, in the three previous posts we have set the scene for containers and covered off some of the impact on the wider IT ecosystem. In this post we look at how containers could affect your current environment and draw a close to this first series (almost…)&lt;/p>
&lt;h2 id="do-i-need-bare-metal-servers-anymore">Do I need bare metal servers anymore?&lt;/h2>
&lt;p>There remains a market for bare metal in enterprises but it&amp;rsquo;s shrinking. The market remains for three primary reasons, maturity of the cloud, legacy applications and security. All things being equal there should be very few reasons that any next generation application would require a bare metal hardware platform. It is worth considering though that the bare metal market for another market segment is accelerating, namely ODM or ‘Other Device Manufacturers’ these are the firms which make the white box servers for some of the public cloud players and also provide the bare bones for functional appliances. Appliances in this context are self-contained purpose-specific combination of compute, storage and network. Recent appliances tend to be based around x64 compatible componentry as a lowest common denominator and widest development environment coverage platform.&lt;/p></description></item><item><title>Compute Workload Abstraction - Part 3, or Rules are there to contain the fun</title><link>https://glennaugustus.com/post/compute-workload-abstraction-part-3-rules-are-there-to-contain-the-fun/</link><pubDate>Mon, 03 Aug 2015 10:12:53 +0000</pubDate><guid>https://glennaugustus.com/post/compute-workload-abstraction-part-3-rules-are-there-to-contain-the-fun/</guid><description>&lt;p>This is part three in a series exploring the topic of compute workload abstraction, in &lt;a href="https://glennaugustus.com/post/compute-workload-abstraction-part-1-or-one-hal-of-a-story/">parts one&lt;/a> and &lt;a href="https://glennaugustus.com/post/compute-workload-abstraction-part-2-or-virtualization-is-dead-long-live-virtualization/">two&lt;/a> we looked at containers and how standardisation helps to build solid services. In this part we consider some of the wider implications on standards, security and licencing.&lt;/p>
&lt;h2 id="doesnt-it-feel-time-for-another-standard">Doesn&amp;rsquo;t it feel time for another standard?&lt;/h2>
&lt;p>Well maybe yes, as we enter the next phase for abstraction we can execute on 4 C’s which expand out as the Commonality of Capability leads to Commoditization in Computing. As each abstraction layer is commoditized it allows the layer above to implement the ‘southbound’ layers in generic API type models, driving the further commoditization and common capability across providers. One of the interesting areas here, and why a standard may be needed, is how the abstracted component can advertise unique capabilities or services – take for example that a particular container or VM could offer certain scale-up features, or proximity to an existing dataset - would it not be of interest to the domain broker to be able to serve not just generic workload execution engines but ones that support the context of the business request? The open standards bodies that come together are clearly excited in this space – any subject that has a high level of cross technology magnetism such as Docker, will iteratively divide and unite the community very quickly, leaving what has been demonstrated in the past many times as the most generally acceptable solution to move forward, not always the best from a subjective viewpoint, but acceptable.&lt;/p></description></item><item><title>Compute Workload Abstraction - Part 2, or Virtualization is dead, long live Virtualization!</title><link>https://glennaugustus.com/post/compute-workload-abstraction-part-2-virtualization-is-dead-long-live-virtualization/</link><pubDate>Mon, 27 Jul 2015 17:40:02 +0000</pubDate><guid>https://glennaugustus.com/post/compute-workload-abstraction-part-2-virtualization-is-dead-long-live-virtualization/</guid><description>&lt;p>This is part two of a series exploring the topic of compute workload abstraction. In &lt;a href="https://glennaugustus.com/post/compute-workload-abstraction-part-1-or-one-hal-of-a-story/">part one&lt;/a> we looked at the unsung hero in virtualization, the HAL, and in this part we dive a little deeper and scratch the surface on the potential benefits for applications.&lt;/p>
&lt;h2 id="is-traditional-virtualization-dead-then">Is traditional virtualization dead, then?&lt;/h2>
&lt;p>We ended part one asking if containers are a traditional virtualization killer, and in my view they are both complementary and competitive. Sure, there will be some ground conceded, but it simply complies with the evolutionary process of what I call the &lt;em>absorption of differentiation into expectation&lt;/em>.&lt;/p></description></item><item><title>Compute Workload Abstraction - Part 1, or One HAL of a Story</title><link>https://glennaugustus.com/post/compute-workload-abstraction-part-1-or-one-hal-of-a-story/</link><pubDate>Thu, 16 Jul 2015 16:24:16 +0000</pubDate><guid>https://glennaugustus.com/post/compute-workload-abstraction-part-1-or-one-hal-of-a-story/</guid><description>&lt;p>This is part of a series of light reading points-of-view to help foster opinion on wide ranging technology topics, and maybe throw in some education along the way. In this series we are looking to provide some insight into the increasingly topical area of workload abstraction. As always, please feel free to comment and get in touch!&lt;/p>
&lt;h1 id="how-virtualization-was-just-the-first-step">How virtualization was just the first step&lt;/h1>
&lt;p>So we all know where all this virtualization came from, right? It’s a common thought that the sole business case for virtualization was derived from the inefficient use of existing hardware resources, and of course this is part of the story. But the unsung hero of virtualization was the usage of a simplified set of resources from the hardware abstraction layer. This meant that when an application was running in a virtual machine there was a good chance that problems in the operating system were down to an exhausted resource or poor configuration, not down to a bad driver or a kernel fault. The combination of vendor testing and almost instant worldwide feedback created an ecosystem where any significant problems were either removed before release or patched before you had a chance to download it anyway! Not all problems were eradicated, but the reduction in variants at the hypervisor level created a far more standard environment in which operating systems and applications could live in relative harmony.&lt;/p></description></item></channel></rss>